{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import pandas as pd\nimport torch\nfrom torch.utils.data import TensorDataset\nfrom tqdm.notebook import tqdm\nfrom transformers import BertForSequenceClassification, BertTokenizer"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Conference</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Innovation in Database Management: Computer Sc...</td>\n      <td>VLDB</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>High performance prime field multiplication fo...</td>\n      <td>ISCAS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>enchanted scissors: a scissor interface for su...</td>\n      <td>SIGGRAPH</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Detection of channel degradation attack by Int...</td>\n      <td>INFOCOM</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Pinning a Complex Network through the Betweenn...</td>\n      <td>ISCAS</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                                               Title Conference\n0  Innovation in Database Management: Computer Sc...       VLDB\n1  High performance prime field multiplication fo...      ISCAS\n2  enchanted scissors: a scissor interface for su...   SIGGRAPH\n3  Detection of channel degradation attack by Int...    INFOCOM\n4  Pinning a Complex Network through the Betweenn...      ISCAS"
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df = pd.read_csv(\"data/title_conference.csv\")\ndf.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u0417\u0430\u043c\u0435\u0442\u0438\u043c, \u0447\u0442\u043e \u043a\u043b\u0430\u0441\u0441\u044b \u043d\u0435\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u044b:"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "ISCAS       864\nINFOCOM     515\nVLDB        423\nWWW         379\nSIGGRAPH    326\nName: Conference, dtype: int64"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df.Conference.value_counts()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Encoding labels"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'VLDB': 0, 'ISCAS': 1, 'SIGGRAPH': 2, 'INFOCOM': 3, 'WWW': 4}"
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "possible_labels = df.Conference.unique()\n\nlabel_dict = {}\nfor index, possible_label in enumerate(possible_labels):\n    label_dict[possible_label] = index\nlabel_dict"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "df[\"label\"] = df.Conference.replace(label_dict)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Train and Vallidation Split\nBecause dataset have unbalanced classes, we split the data in stratified fashion."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "C:\\Users\\ds13\\AppData\\Local\\Temp\\ipykernel_39904\\2360066991.py:11: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n  df.data_type = [\"not_set\"] * df.shape[0]\n"
                },
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>Title</th>\n    </tr>\n    <tr>\n      <th>Conference</th>\n      <th>label</th>\n      <th>data_type</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">INFOCOM</th>\n      <th rowspan=\"2\" valign=\"top\">3</th>\n      <th>train</th>\n      <td>438</td>\n    </tr>\n    <tr>\n      <th>val</th>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">ISCAS</th>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th>train</th>\n      <td>734</td>\n    </tr>\n    <tr>\n      <th>val</th>\n      <td>130</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">SIGGRAPH</th>\n      <th rowspan=\"2\" valign=\"top\">2</th>\n      <th>train</th>\n      <td>277</td>\n    </tr>\n    <tr>\n      <th>val</th>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">VLDB</th>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>train</th>\n      <td>359</td>\n    </tr>\n    <tr>\n      <th>val</th>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">WWW</th>\n      <th rowspan=\"2\" valign=\"top\">4</th>\n      <th>train</th>\n      <td>322</td>\n    </tr>\n    <tr>\n      <th>val</th>\n      <td>57</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                            Title\nConference label data_type       \nINFOCOM    3     train        438\n                 val           77\nISCAS      1     train        734\n                 val          130\nSIGGRAPH   2     train        277\n                 val           49\nVLDB       0     train        359\n                 val           64\nWWW        4     train        322\n                 val           57"
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from sklearn.model_selection import train_test_split  # noqa: 402\n\nX_train, X_val, y_train, y_val = train_test_split(\n    df.index.values,\n    df.label.values,\n    test_size=0.15,\n    random_state=42,\n    stratify=df.label.values,\n)\n\ndf.data_type = [\"not_set\"] * df.shape[0]\n\ndf.loc[X_train, \"data_type\"] = \"train\"\ndf.loc[X_val, \"data_type\"] = \"val\"\n\ndf.groupby([\"Conference\", \"label\", \"data_type\"]).count()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## BertTokenizer"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
                }
            ],
            "source": "BERT_MODEL_TYPE = \"bert-base-uncased\"\n\n\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_TYPE, do_lower_case=True)\n\n\ndef batch_encode_plus(data):\n    return tokenizer.batch_encode_plus(\n        data.Title.values,\n        # Sequences will be encoded with th especial tokens relative to their model.\n        add_special_tokens=True,\n        # Return attention mask according to specific tokenizer.\n        return_attention_mask=True,\n        pad_to_max_length=True,\n        max_length=256,  # Limit just in case.\n        return_tensors=\"pt\",  # Return pytorch compatible tensors.\n    )\n\n\ntrain_data = df[df[\"data_type\"] == \"train\"]\nencoded_data_train = batch_encode_plus(train_data)\n\nval_data = df[df[\"data_type\"] == \"val\"]\nencoded_data_val = batch_encode_plus(val_data)\n\ninput_ids_train = encoded_data_train[\"input_ids\"]\nattention_masks_train = encoded_data_train[\"attention_mask\"]\nlabels_train = torch.tensor(train_data.label.values)\n\ninput_ids_val = encoded_data_val[\"input_ids\"]\nattention_masks_val = encoded_data_val[\"attention_mask\"]\nlabels_val = torch.tensor(val_data.label.values)\n\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## BERT Pre-trained model"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                }
            ],
            "source": "model = BertForSequenceClassification.from_pretrained(\n    BERT_MODEL_TYPE,\n    num_labels=len(label_dict),\n    output_attentions=False,\n    output_hidden_states=False,\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Data Loaders\nLet's combine a dataset and a sampler to data loader that provides an iterable\nover the given dataset."
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler  # noqa: 402\n\nBATCH_SIZE = 3\n\ndataloader_train = DataLoader(\n    dataset_train, sampler=RandomSampler(dataset_train), batch_size=BATCH_SIZE\n)\n\ndataloader_val = DataLoader(\n    dataset_val, sampler=RandomSampler(dataset_val), batch_size=BATCH_SIZE\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Optimizer & Scheduler\n1. We should define parameters to optimize in iterable.\n2. Then specify optmizer-specific options such as epochs, learning_rate...\n3. Create a schedule with a learning rate that first inreases linearly from\n0 to the initial learning rate set in the optimizer (a.k.a. warm up period) and then\ndecreases linearly from the initial learning rate to 0."
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "C:\\Users\\ds13\\.bookmarks\\Diploma@RepoAnalyzer__\\repolier\\.venv_win\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n"
                }
            ],
            "source": "from transformers import AdamW, get_linear_schedule_with_warmup  # noqa: 402\n\nLEARNING_RATE = 1e-5\nEPSILON = 1e-8\nEPOCHS = 5  # Depends on dataset.\n\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPSILON)\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * EPOCHS\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Performance mentrics\nWe will use f1 and accuracy per class."
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import numpy as np  # noqa: 402\nfrom sklearn.metrics import f1_score  # noqa: 402\n\n\ndef get_f1_score(predictions, labels):\n    predictions_flattened = np.argmax(predictions, axis=1).flatten()\n    labels_flattened = labels.flatten()\n\n    return f1_score(labels_flattened, predictions_flattened, average=\"weighted\")"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def accuracy_per_class(predictions, labels):\n    # Inverse the dictionary.\n    labels_lookup_table = {v: k for k, v in label_dict.items()}\n\n    predictions_flattened = np.argmax(predictions, axis=1).flatten()\n    labels_flattened = labels.flatten()\n\n    for label in np.unique(labels_flattened):\n        y_predicted = predictions_flattened[labels_flattened == label]\n        y_true = labels_flattened[labels_flattened == label]\n\n        print(f\"Class: {labels_lookup_table[label]}\")\n        print(f\"Accuracy: {len(y_predicted[y_predicted==label])}/{len(y_true)}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Training loop"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "cuda\n"
                }
            ],
            "source": "import random  # noqa: 402\n\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(device)"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b8be8d33e4d6430d912a413ca080b86e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "Epoch 1:   0%|          | 0/710 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nEpoch 1\nTraining loss: 0.8888539495497523\nValidation loss: 0.5301494788348912\nF1 Score (weighted): 0.8349282731792835\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "Epoch 2:   0%|          | 0/710 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nEpoch 2\nTraining loss: 0.5080354237650245\nValidation loss: 0.6182522111893853\nF1 Score (weighted): 0.8465429330535159\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "Epoch 3:   0%|          | 0/710 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nEpoch 3\nTraining loss: 0.3500029952366981\nValidation loss: 0.6915738689620787\nF1 Score (weighted): 0.8477420006215807\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "Epoch 4:   0%|          | 0/710 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nEpoch 4\nTraining loss: 0.23502694387300413\nValidation loss: 0.8227984245265058\nF1 Score (weighted): 0.8365971847893648\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "Epoch 5:   0%|          | 0/710 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nEpoch 5\nTraining loss: 0.16323284446893088\nValidation loss: 0.789898806753742\nF1 Score (weighted): 0.8472826151498132\n"
                }
            ],
            "source": "from pathlib import Path  # noqa: 402\n\nPath(\"models\").mkdir(parents=True, exist_ok=True)\n\n\ndef map_batch_to_inputs(batch):\n    return {\n        \"input_ids\": batch[0],\n        \"attention_mask\": batch[1],\n        \"labels\": batch[2],\n    }\n\n\ndef evaluate(dataloader_val):\n    model.eval()\n\n    loss_val_total = 0\n    predictions, true_vals = [], []\n\n    for batch in dataloader_val:\n        batch = tuple(b.to(device) for b in batch)\n\n        inputs = map_batch_to_inputs(batch)\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs[\"labels\"].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n\n    loss_val_avg = loss_val_total / len(dataloader_val)\n\n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n\n    return loss_val_avg, predictions, true_vals\n\n\nfor epoch in tqdm(range(1, EPOCHS + 1)):\n    model.train()\n\n    loss_train_total = 0\n\n    progress_bar = tqdm(\n        dataloader_train, desc=\"Epoch {:1d}\".format(epoch), leave=False, disable=False\n    )\n\n    for batch in progress_bar:\n        model.zero_grad()\n\n        batch = tuple(b.to(device) for b in batch)\n\n        inputs = map_batch_to_inputs(batch)\n\n        outputs = model(**inputs)\n\n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n        progress_bar.set_postfix(\n            {\"training_loss\": \"{:.3f}\".format(loss.item() / len(batch))}\n        )\n\n    torch.save(model.state_dict(), f\"models/finetuned_BERT_epoch_{epoch}.model\")\n\n    tqdm.write(f\"\\nEpoch {epoch}\")\n\n    loss_train_avg = loss_train_total / len(dataloader_train)\n    tqdm.write(f\"Training loss: {loss_train_avg}\")\n\n    val_loss, predictions, true_vals = evaluate(dataloader_val)\n    val_f1 = get_f1_score(predictions, true_vals)\n    tqdm.write(f\"Validation loss: {val_loss}\")\n    tqdm.write(f\"F1 Score (weighted): {val_f1}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Loading and evaluating the model"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                },
                {
                    "data": {
                        "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model = BertForSequenceClassification.from_pretrained(\n    BERT_MODEL_TYPE,\n    num_labels=len(label_dict),\n    output_attentions=False,\n    output_hidden_states=False,\n)\n\nmodel.to(device)"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "<All keys matched successfully>"
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.load_state_dict(\n    torch.load(\"models/finetuned_BERT_epoch_1.model\", map_location=torch.device(\"cuda\"))\n)"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[[ 3.0668375  -1.4040668  -1.1406454  -0.40617484  0.13424563]\n [-0.20288248  0.817448    2.4179237  -1.461867   -0.2450478 ]\n [-0.09810138 -1.5999974  -1.0052575   0.30795276  2.0977244 ]\n ...\n [ 2.728063   -0.6249323   0.00463311 -1.4429905   0.21042904]\n [ 2.5265696  -1.1070975  -1.8461831   0.8716704  -0.29535842]\n [ 0.5891698  -1.5345939  -0.6063291  -1.2490883   2.9028754 ]]\n[0 2 4 1 3 1 2 1 2 1 0 1 4 1 1 0 4 1 1 1 2 1 4 3 3 3 2 1 3 1 1 3 3 3 4 1 0\n 0 2 1 1 1 3 3 3 4 0 2 1 1 3 3 4 1 1 3 4 2 0 0 1 3 3 3 0 0 3 1 1 1 1 0 2 1\n 0 2 1 3 3 1 2 4 1 2 2 4 2 0 4 4 4 3 1 4 2 3 0 4 2 3 0 1 1 1 0 1 1 2 4 0 0\n 3 3 0 1 2 3 0 4 1 0 1 1 4 0 3 3 0 2 1 1 2 1 3 1 2 1 1 0 1 1 3 3 3 0 2 2 0\n 2 4 3 2 3 1 1 1 3 2 0 3 1 2 1 0 1 1 1 3 2 1 1 0 2 4 1 1 1 2 3 3 1 1 1 3 0\n 0 4 2 4 3 4 4 1 3 1 1 3 3 1 2 2 1 1 3 4 1 0 0 4 4 4 1 2 4 0 4 4 3 2 0 0 0\n 1 1 4 4 2 1 4 4 2 2 3 1 1 4 1 1 0 1 2 3 0 4 1 4 4 3 1 4 1 0 1 4 2 1 1 0 4\n 0 4 1 3 1 1 3 3 1 1 1 0 1 1 3 0 4 1 1 4 0 1 0 4 3 1 1 0 2 1 0 1 1 4 0 1 1\n 1 4 3 1 4 1 3 1 4 0 3 1 1 0 3 4 1 3 3 3 2 4 2 1 3 0 3 1 1 4 3 3 4 0 1 2 3\n 0 1 1 3 2 0 1 0 0 1 0 2 0 3 1 1 1 3 3 1 1 1 4 1 2 3 3 1 0 0 2 1 1 0 2 3 3\n 3 4 3 3 0 0 4]\nClass: VLDB\nAccuracy: 49/64\nClass: ISCAS\nAccuracy: 117/130\nClass: SIGGRAPH\nAccuracy: 42/49\nClass: INFOCOM\nAccuracy: 56/77\nClass: WWW\nAccuracy: 50/57\n"
                }
            ],
            "source": "_, predictions, true_vals = evaluate(dataloader_val)\naccuracy_per_class(predictions, true_vals)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
