{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import pandas as pd\nimport torch\nfrom torch.utils.data import TensorDataset\nfrom tqdm.notebook import tqdm\nfrom transformers import BertForSequenceClassification, BertTokenizer"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Conference</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Innovation in Database Management: Computer Sc...</td>\n      <td>VLDB</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>High performance prime field multiplication fo...</td>\n      <td>ISCAS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>enchanted scissors: a scissor interface for su...</td>\n      <td>SIGGRAPH</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Detection of channel degradation attack by Int...</td>\n      <td>INFOCOM</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Pinning a Complex Network through the Betweenn...</td>\n      <td>ISCAS</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                                               Title Conference\n0  Innovation in Database Management: Computer Sc...       VLDB\n1  High performance prime field multiplication fo...      ISCAS\n2  enchanted scissors: a scissor interface for su...   SIGGRAPH\n3  Detection of channel degradation attack by Int...    INFOCOM\n4  Pinning a Complex Network through the Betweenn...      ISCAS"
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df = pd.read_csv(\"data/title_conference.csv\")\ndf.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u0417\u0430\u043c\u0435\u0442\u0438\u043c, \u0447\u0442\u043e \u043a\u043b\u0430\u0441\u0441\u044b \u043d\u0435\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u044b:"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "ISCAS       864\nINFOCOM     515\nVLDB        423\nWWW         379\nSIGGRAPH    326\nName: Conference, dtype: int64"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df.Conference.value_counts()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Encoding labels"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "ename": "AttributeError",
                    "evalue": "'LabelEncoder' object has no attribute 'classes_'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[0;32m      3\u001b[0m le \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mLabelEncoder()\n\u001b[1;32m----> 4\u001b[0m labels_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\n\u001b[0;32m      5\u001b[0m labels_dict \u001b[38;5;241m=\u001b[39m {l: i \u001b[38;5;28;01mfor\u001b[39;00m (i, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels_encoded)}\n\u001b[0;32m      6\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mConference\u001b[38;5;241m.\u001b[39mreplace(labels_dict)\n",
                        "\u001b[1;31mAttributeError\u001b[0m: 'LabelEncoder' object has no attribute 'classes_'"
                    ]
                }
            ],
            "source": "from sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\nlabels_encoded = le.fit(df.Conference).classes_\nlabels_dict = {l: i for (i, l) in enumerate(labels_encoded)}\ndf[\"label\"] = df.Conference.replace(labels_dict)\ndf"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Train and Vallidation Split\nBecause dataset have unbalanced classes, we split the data in stratified fashion."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from sklearn.model_selection import train_test_split  # noqa: 402\n\nX_train, X_val, y_train, y_val = train_test_split(\n    df.index.values,\n    df.label.values,\n    test_size=0.15,\n    random_state=42,\n    stratify=df.label.values,\n)\n\ndf.data_type = [\"not_set\"] * df.shape[0]\n\ndf.loc[X_train, \"data_type\"] = \"train\"\ndf.loc[X_val, \"data_type\"] = \"val\"\n\ndf.groupby([\"Conference\", \"label\", \"data_type\"]).count()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## BertTokenizer"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "BERT_MODEL_TYPE = \"bert-base-uncased\"\n\n\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_TYPE, do_lower_case=True)\n\n\ndef batch_encode_plus(data):\n    return tokenizer.batch_encode_plus(\n        data.Title.values,\n        # Sequences will be encoded with th especial tokens relative to their model.\n        add_special_tokens=True,\n        # Return attention mask according to specific tokenizer.\n        return_attention_mask=True,\n        pad_to_max_length=True,\n        max_length=256,  # Limit just in case.\n        return_tensors=\"pt\",  # Return pytorch compatible tensors.\n    )\n\n\ntrain_data = df[df[\"data_type\"] == \"train\"]\nencoded_data_train = batch_encode_plus(train_data)\n\nval_data = df[df[\"data_type\"] == \"val\"]\nencoded_data_val = batch_encode_plus(val_data)\n\ninput_ids_train = encoded_data_train[\"input_ids\"]\nattention_masks_train = encoded_data_train[\"attention_mask\"]\nlabels_train = torch.tensor(train_data.label.values)\n\ninput_ids_val = encoded_data_val[\"input_ids\"]\nattention_masks_val = encoded_data_val[\"attention_mask\"]\nlabels_val = torch.tensor(val_data.label.values)\n\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## BERT Pre-trained model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "model = BertForSequenceClassification.from_pretrained(\n    BERT_MODEL_TYPE,\n    num_labels=len(labels_encoded),\n    output_attentions=False,\n    output_hidden_states=False,\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Data Loaders\nLet's combine a dataset and a sampler to data loader that provides an iterable\nover the given dataset."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler  # noqa: 402\n\nBATCH_SIZE = 3\n\ndataloader_train = DataLoader(\n    dataset_train, sampler=RandomSampler(dataset_train), batch_size=BATCH_SIZE\n)\n\ndataloader_val = DataLoader(\n    dataset_val, sampler=RandomSampler(dataset_val), batch_size=BATCH_SIZE\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Optimizer & Scheduler\n1. We should define parameters to optimize in iterable.\n2. Then specify optmizer-specific options such as epochs, learning_rate...\n3. Create a schedule with a learning rate that first inreases linearly from\n0 to the initial learning rate set in the optimizer (a.k.a. warm up period) and then\ndecreases linearly from the initial learning rate to 0."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from transformers import AdamW, get_linear_schedule_with_warmup  # noqa: 402\n\nLEARNING_RATE = 1e-5\nEPSILON = 1e-8\nEPOCHS = 5  # Depends on dataset.\n\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPSILON)\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * EPOCHS\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Performance metrics\nWe will use f1 and accuracy per class."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import numpy as np  # noqa: 402\nfrom sklearn.metrics import f1_score  # noqa: 402\n\n\ndef get_f1_score(predictions, labels):\n    predictions_flattened = np.argmax(predictions, axis=1).flatten()\n    labels_flattened = labels.flatten()\n\n    return f1_score(labels_flattened, predictions_flattened, average=\"weighted\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def accuracy_per_class(predictions, labels):\n    # Inverse the dictionary.\n    labels_lookup_table = {v: k for k, v in labels_dict.items()}\n\n    predictions_flattened = np.argmax(predictions, axis=1).flatten()\n    labels_flattened = labels.flatten()\n\n    for label in np.unique(labels_flattened):\n        y_predicted = predictions_flattened[labels_flattened == label]\n        y_true = labels_flattened[labels_flattened == label]\n\n        print(f\"Class: {labels_lookup_table[label]}\")\n        print(f\"Accuracy: {len(y_predicted[y_predicted==label])}/{len(y_true)}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Training loop"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import random  # noqa: 402\n\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(device)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from pathlib import Path  # noqa: 402\nfrom torch import nn  # noqa: 402\n\nPath(\"models\").mkdir(parents=True, exist_ok=True)\n\n\ndef map_batch_to_inputs(batch):\n    return {\n        \"input_ids\": batch[0],\n        \"attention_mask\": batch[1],\n        \"labels\": batch[2],\n    }\n\n\ndef evaluate(dataloader_val):\n    model.eval()\n\n    loss_val_total = 0\n    predictions, true_vals = [], []\n\n    for batch in dataloader_val:\n        batch = tuple(b.to(device) for b in batch)\n\n        inputs = map_batch_to_inputs(batch)\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs[\"labels\"].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n\n    loss_val_avg = loss_val_total / len(dataloader_val)\n\n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n\n    return loss_val_avg, predictions, true_vals\n\n\nfor epoch in tqdm(range(1, EPOCHS + 1)):\n    model.train()\n\n    loss_train_total = 0\n\n    progress_bar = tqdm(\n        dataloader_train, desc=\"Epoch {:1d}\".format(epoch), leave=False, disable=False\n    )\n\n    for batch in progress_bar:\n        model.zero_grad()\n\n        batch = tuple(b.to(device) for b in batch)\n\n        inputs = map_batch_to_inputs(batch)\n\n        outputs = model(**inputs)\n\n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n        progress_bar.set_postfix(\n            {\"training_loss\": \"{:.3f}\".format(loss.item() / len(batch))}\n        )\n\n    torch.save(model.state_dict(), f\"models/finetuned_BERT_epoch_{epoch}.model\")\n\n    tqdm.write(f\"\\nEpoch {epoch}\")\n\n    loss_train_avg = loss_train_total / len(dataloader_train)\n    tqdm.write(f\"Training loss: {loss_train_avg}\")\n\n    val_loss, predictions, true_vals = evaluate(dataloader_val)\n    val_f1 = get_f1_score(predictions, true_vals)\n    tqdm.write(f\"Validation loss: {val_loss}\")\n    tqdm.write(f\"F1 Score (weighted): {val_f1}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Loading and evaluating the model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "model = BertForSequenceClassification.from_pretrained(\n    BERT_MODEL_TYPE,\n    num_labels=len(labels_encoded),\n    output_attentions=False,\n    output_hidden_states=False,\n)\n\nmodel.to(device)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "model.load_state_dict(\n    torch.load(\"models/finetuned_BERT_epoch_1.model\", map_location=torch.device(\"cuda\"))\n)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "_, predictions, true_vals = evaluate(dataloader_val)\naccuracy_per_class(predictions, true_vals)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}